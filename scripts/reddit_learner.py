#!/usr/bin/env python3
"""
Reddit çƒ­é—¨å†…å®¹å­¦ä¹ å™¨
è‡ªåŠ¨é‡‡é›† Reddit çƒ­é—¨å¸–å­ï¼Œåˆ†æå¹¶ä¿å­˜åˆ°è®°å¿†åº“
"""

import json
import sys
sys.path.append('/home/ubuntu/.openclaw/workspace/memory')
from memory_system import MemorySystem
from datetime import datetime
import subprocess

def fetch_reddit_hot_json(subreddit, limit=10):
    """ä½¿ç”¨ curl è·å– Reddit çƒ­é—¨ï¼ˆJSON æ ¼å¼ï¼‰"""
    url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit={limit}"
    headers = "User-Agent: OpenClaw-Bot/1.0"

    try:
        result = subprocess.run(
            f"curl -s -A '{headers}' '{url}'",
            shell=True,
            capture_output=True,
            text=True,
            timeout=30
        )

        if result.returncode == 0:
            data = json.loads(result.stdout)
            posts = []

            for child in data['data']['children']:
                post = child['data']
                posts.append({
                    'title': post['title'],
                    'url': f"https://reddit.com{post['permalink']}",
                    'score': post['score'],
                    'num_comments': post['num_comments'],
                    'author': post.get('author', '[deleted]'),
                    'created': datetime.fromtimestamp(post['created_utc']).isoformat()
                })

            return posts
        else:
            print(f"âŒ è·å–å¤±è´¥: {result.stderr}")
            return []
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return []

def analyze_post_value(post):
    """åˆ†æå¸–å­ä»·å€¼ï¼ˆç®€å•è§„åˆ™ï¼‰"""
    # è§„åˆ™ï¼šé«˜èµ + é«˜è¯„è®º = é«˜ä»·å€¼
    if post['score'] > 100 and post['num_comments'] > 50:
        return "é«˜ä»·å€¼"
    elif post['score'] > 50 or post['num_comments'] > 20:
        return "ä¸­ç­‰ä»·å€¼"
    else:
        return "ä½ä»·å€¼"

def learn_from_subreddit(subreddit, limit=20):
    """ä»æŒ‡å®š subreddit å­¦ä¹ """
    print(f"\nğŸ” é‡‡é›† r/{subreddit} çƒ­é—¨å†…å®¹...")

    posts = fetch_reddit_hot_json(subreddit, limit)

    if not posts:
        print(f"âš ï¸  r/{subreddit} æ²¡æœ‰è·å–åˆ°å†…å®¹")
        return []

    print(f"âœ… è·å–åˆ° {len(posts)} ä¸ªå¸–å­")

    # åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ
    memory = MemorySystem()
    learned = []

    for post in posts:
        # åˆ†æä»·å€¼
        value = analyze_post_value(post)

        if value != "ä½ä»·å€¼":
            # ä¿å­˜åˆ°è®°å¿†åº“
            knowledge_text = f"""
æ ‡é¢˜ï¼š{post['title']}
æ¥æºï¼šr/{subreddit}
é“¾æ¥ï¼š{post['url']}
çƒ­åº¦ï¼š{post['score']} upvotes, {post['num_comments']} comments
ä½œè€…ï¼š{post['author']}
æ—¶é—´ï¼š{post['created']}
ä»·å€¼ï¼š{value}
"""

            memory.add_knowledge(
                text=knowledge_text,
                category=f"Reddit-{subreddit}",
                source="Reddit",
                metadata={
                    'url': post['url'],
                    'score': post['score'],
                    'value': value
                }
            )

            learned.append(post)
            print(f"  âœ… {post['title'][:50]}... [{value}]")

    return learned

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¥ Reddit çƒ­é—¨å†…å®¹å­¦ä¹ å™¨")
    print("=" * 60)

    # è¦å­¦ä¹ çš„ subreddit
    subreddits = [
        "MachineLearning",
        "artificial",
        "ChatGPT",
        "OpenAI",
        "LocalLLaMA"
    ]

    total_learned = []

    for subreddit in subreddits:
        learned = learn_from_subreddit(subreddit, limit=10)
        total_learned.extend(learned)

    # ç»Ÿè®¡
    print("\nğŸ“Š å­¦ä¹ ç»Ÿè®¡ï¼š")
    print(f"  æ€»å…±å­¦ä¹ ï¼š{len(total_learned)} ä¸ªå¸–å­")

    # å¯¼å‡ºåˆ° Obsidian
    print("\nğŸ“ å¯¼å‡ºåˆ° Obsidian...")
    memory = MemorySystem()
    export_file = memory.export_to_obsidian()
    print(f"  å¯¼å‡ºæ–‡ä»¶ï¼š{export_file}")

    print("\nâœ… å­¦ä¹ å®Œæˆï¼")

if __name__ == "__main__":
    main()
